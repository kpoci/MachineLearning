# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1-HAUBq9NdsHOLCb3Z3Hr5CTlzwvZerBU
"""

import pandas as pd


data = pd.read_csv('Conversational_data.csv')

print(data.head(15))

import pandas as pd
import torch
from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments
from sklearn.model_selection import train_test_split
from transformers import pipeline

data = pd.read_csv('Conversational_data.csv')


print(data.head(10))


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels=3)


inputs = tokenizer(
    data['user_input'].tolist(),
    padding=True,
    truncation=True,
    return_tensors="pt"
)


labels = torch.tensor([0 if intent == "book_flight" else 1 if intent == "get_weather" else 2 for intent in data['intent']])


class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)


dataset = CustomDataset(inputs, labels)


train_dataset, test_dataset = torch.utils.data.random_split(dataset, [int(0.8*len(dataset)), len(dataset) - int(0.8*len(dataset))])


training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=64,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=test_dataset,
)


trainer.train()


ner_model = pipeline("ner", grouped_entities=True)  # Model with grouped entities

feedback_data = []


def predict_intent(user_input):

    tokens = tokenizer(user_input, padding=True, truncation=True, return_tensors="pt")


    tokens = {k: v.to(model.device) for k, v in tokens.items()}

    with torch.no_grad():
        outputs = model(**tokens)
        predictions = torch.argmax(outputs.logits, dim=-1).item()


    intent_mapping = {0: "book_flight", 1: "get_weather", 2: "other_intent"}
    predicted_intent = intent_mapping[predictions]
    return predicted_intent


def chatbot_response(user_input):

    predicted_intent = predict_intent(user_input)


    print(f"Predicted Intent: {predicted_intent}")


    if predicted_intent == "book_flight":
        response = "It looks like you want to book a flight. Let me assist you with that."
    elif predicted_intent == "get_weather":
        response = "You are asking about the weather. Let me provide the latest forecast."
    else:
        response = "I'm not sure what you're asking. Could you please clarify?"


    print(f"Response: {response}")


    entities = ner_model(user_input)


    print("Entities found:")
    for entity in entities:
        print({
            'word': entity['word'],
            'score': entity['score'],
            'start': entity['start'],
            'end': entity['end'],
            'entity': entity['entity_group']
        })

    feedback_data.append({
        'user_input': user_input,
        'Entities': entities,
        'Intent': predicted_intent
    })


    feedback = int(input("Rate the response (1-5): "))
    feedback_data.append({'user_input': user_input, 'response': response, 'feedback': feedback})


    pd.DataFrame(feedback_data).to_csv('feedback_data.csv', index=False)


user_input = input("User: ")
chatbot_response(user_input)

user_input = input("\nInput: ")
chatbot_response(user_input)
